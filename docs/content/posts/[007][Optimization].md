# Optimization - What's the point?
## Is it really worth it?

Optimizing our code can be a lot of work, but before we get into some examples lets have a look at what we get for free.  Below are the execution times of several operations using different implementations.  The left side shows the times with compiler optimizations turned on, and the right side shows the times with compiler optimizations disabled.

![alt text](https://github.com/adam-lafontaine/CMS/raw/master/img/%5B007%5D/example_output.png)

In general, the compiler-optimized versions are faster and the performance gains are quite significant.  The only exception being the "8 wide" version of the Fused Muliply-Add example.  In the "SIMD" section, the times decrease with the "width" when unoptimized and the times are much closer together in the optimized versions.  The "8 wide" versions are also the slowest when optimized.  When unoptimized, the "8 wide" versions are just as fast as the optimized versions.


### SIMD - Single Instruction Multiple Data

Chip manufacturers provide special instructions that can be used to perform the same operation on multiple data elements at a time.  This can greatly reduce the amount of time it takes to process large amounts of data.  It is not multithreading.  The array elements are "packed" or "vectorized" into registers that hold multiple values, and the operation (e.g. add, subtract, multiply etc.) is performed on all of them together.  Below is an example of adding the values of two arrays together.

```cpp
// header files for Intel intrinsics
#include <immintrin.h>
#include <xmmintrin.h>


using r32 = float;


void add_4_wide(r32* arr_a, r32* arr_b, r32* arr_dst, size_t N)
{
    // add elements of arr_a to elements of arr_b and store the result in arr_dst

	__m128 wa; // 128 bits wide, holds 4 32 bit floats
	__m128 wb;
	__m128 wdst;

	// loop over the arrays, 4 elements at a time
	for (size_t i = 0; i < N; i += 4)
	{
		// get starting pointers for the current 4 elements
		auto a = arr_a + i;
		auto b = arr_b + i;
		auto dst = arr_dst + i;

		// load values from arr_a and arr_b into the registers
		wa = _mm_load_ps(a);
		wb = _mm_load_ps(b);

		// add the values in a and b together
		wdst = _mm_add_ps(wa, wb);

        /* 

          wa: [ 1.0f, 2.0f, 3.0f, 4.0f ]
          wb: [ 5.0f, 6.0f, 7.0f, 8.0f ]

        wdst: [ 6.0f, 8.0f, 10.0f, 12.0f ]

        */

		// store the results at this location in arr_dst
		_mm_store_ps(dst, wdst);
	}
}
```

There is no standard API for these types of operations.  Each chip manufacturer has its own intrinsics and not all chips have the same functionality.  In order to use them, we need to be sure that the device running our program has the instructions available.

The SIMD examples in this post use Intel intrinsics compiled with Visual Studio (output above).  The "1 wide" examples process the arrays in the usual way one element at a time.  The "4 wide" examples process 4 elements at a time and the "8 wide" examples process 8 elements.

SIMD is a large topic and there are a lot of instructions available.  Check out the [Intel Intrinsics Guide](https://intel.com/content/www/us/en/docs/intrinsics-guide/index.html) if you would like to feel overwhelmed.


### Example - SIMD Multiply

The first example in the output is similar to the add example above except that the values are multiplied.  Doing this normally is pretty straightforward.

```cpp
void multiply_1_wide(r32* arr_a, r32* arr_b, r32* arr_dst, size_t N)
{
	for (size_t i = 0; i < N; ++i)
	{
		arr_dst[i] = arr_a[i] * arr_b[i];
	}
}
```

Multiplying 4 elements at a time is very similar to adding them.  The only change is a function call.

```cpp
void multiply_4_wide(r32* arr_a, r32* arr_b, r32* arr_dst, size_t N)
{
	__m128 wa;
	__m128 wb;
	__m128 wdst;

	for (size_t i = 0; i < N; i += 4)
	{
		auto a = arr_a + i;
		auto b = arr_b + i;
		auto dst = arr_dst + i;

		wa = _mm_load_ps(a);
		wb = _mm_load_ps(b);

		wdst = _mm_mul_ps(wa, wb);

		_mm_store_ps(dst, wdst);
	}
}
```

In order to do the same with 8 elements we need to use registers that are 256 bits (8 x 32) wide.

```cpp
void multiply_8_wide(r32* arr_a, r32* arr_b, r32* arr_dst, size_t N)
{
	__m256 wa;
	__m256 wb;
	__m256 wdst;

	for (size_t i = 0; i < N; i += 8)
	{
		auto a = arr_a + i;
		auto b = arr_b + i;
		auto dst = arr_dst + i;

		wa = _mm256_load_ps(a);
		wb = _mm256_load_ps(b);

		wdst = _mm256_mul_ps(wa, wb);

		_mm256_store_ps(dst, wdst);
	}
}
```

The output is generated by initializing three arrays, calling each function on them and timing how long each one takes to execute.

```cpp
#include <cstdio>

using r64 = double;


void fill_array(r32* arr, r32 val, size_t N)
{    
	for (size_t i = 0; i < N; ++i)
	{
		arr[i] = val;
	}
}


void multiply()
{
	printf("\nMultiply\n");

	Stopwatch sw;
	r64 time_ms = 0.0;

	size_t const N = 80'000'000;

	auto arr_a = (r32*)malloc(N * sizeof(r32));
	auto arr_b = (r32*)malloc(N * sizeof(r32));
	auto arr_dst = (r32*)malloc(N * sizeof(r32));

	fill_array(arr_a, 3.0f, N);
	fill_array(arr_b, 2.0f, N);
	fill_array(arr_dst, 0.0f, N);

	auto const report_and_reset = [&](int width)
	{
		time_ms = sw.get_time_milli();
		printf("%d wide time: %f\n", width, time_ms);

		for (size_t i = 0; i < N; ++i)
		{
			if (arr_dst[i] != 6.0f)
			{
				printf("!!! multiply error !!!\n");
				break;
			}
		}

		fill_array(arr_dst, 0.0f, N);
	};

	sw.start();

	multiply_1_wide(arr_a, arr_b, arr_dst, N);

	report_and_reset(1);

	sw.start();

	multiply_4_wide(arr_a, arr_b, arr_dst, N);

	report_and_reset(4);

	sw.start();

	multiply_8_wide(arr_a, arr_b, arr_dst, N);

	report_and_reset(8);

	free(arr_a);
	free(arr_b);
	free(arr_dst);
}
```

Note: The Stopwatch class is available in this [previous post](https://almostalwaysauto.com/posts/parallelism-for-free)

![alt text](https://github.com/adam-lafontaine/CMS/raw/master/img/%5B007%5D/simd_multiply.png)

We would expect the 8 wide version to be fastest because it has the fewest total operations to perform.  This is the case when the compiler does not apply any optimizations of its own.  With compiler optimizations enabled, all three versions are pretty much the same with the 8 wide version being slightly slower than the other two.  Also, the unoptimized 8 wide version is just as fast as the optimized versions.


### Example - SIMD Fused Multiply-Add

The fused multiply-add operation is an interesting one because it is two operations done in one step.  It is the multiplication of two numbers followed by adding a third. The equation for a straight line (y = mx + b) would be a fused multiply-add.

The default implementation is done like so.

```cpp
void fmadd_1_wide(r32* arr_a, r32* arr_b, r32* arr_c, r32* arr_dst, size_t N)
{
	for (size_t i = 0; i < N; ++i)
	{
		arr_dst[i] = arr_a[i] * arr_b[i] + arr_c[i];
	}
}
```

The SIMD versions only require a single call.

```cpp
void fmadd_4_wide(r32* arr_a, r32* arr_b, r32* arr_c, r32* arr_dst, size_t N)
{
	__m128 wa;
	__m128 wb;
	__m128 wc;
	__m128 wdst;

	for (size_t i = 0; i < N; i += 4)
	{
		auto a = arr_a + i;
		auto b = arr_b + i;
		auto c = arr_c + i;
		auto dst = arr_dst + i;

		wa = _mm_load_ps(a);
		wb = _mm_load_ps(b);
		wc = _mm_load_ps(c);

		wdst = _mm_fmadd_ps(wa, wb, wc);

		_mm_store_ps(dst, wdst);
	}
}


void fmadd_8_wide(r32* arr_a, r32* arr_b, r32* arr_c, r32* arr_dst, size_t N)
{
	__m256 wa;
	__m256 wb;
	__m256 wc;
	__m256 wdst;

	for (size_t i = 0; i < N; i += 8)
	{
		auto a = arr_a + i;
		auto b = arr_b + i;
		auto c = arr_c + i;
		auto dst = arr_dst + i;

		wa = _mm256_load_ps(a);
		wb = _mm256_load_ps(b);
		wc = _mm256_load_ps(c);

		wdst = _mm256_fmadd_ps(wa, wb, wc);

		_mm256_store_ps(dst, wdst);
	}
}
```

Here is the function used to generate the output.

```cpp
void fused_multiply_add()
{
	printf("\nFused Multiply-Add\n");

	Stopwatch sw;
	r64 time_ms = 0.0;

	size_t const N = 80'000'000;

	auto arr_a = (r32*)malloc(N * sizeof(r32));
	auto arr_b = (r32*)malloc(N * sizeof(r32));
	auto arr_c = (r32*)malloc(N * sizeof(r32));
	auto arr_dst = (r32*)malloc(N * sizeof(r32));

	fill_array(arr_a, 3.0f, N);
	fill_array(arr_b, 2.0f, N);
	fill_array(arr_c, 1.0f, N);
	fill_array(arr_dst, 0.0f, N);

	auto const report_and_reset = [&](int width)
	{
		time_ms = sw.get_time_milli();
		printf("%d wide time: %f\n", width, time_ms);

		for (size_t i = 0; i < N; ++i)
		{
			if (arr_dst[i] != 7.0f)
			{
				printf("!!! fmadd error !!!\n");
				break;
			}
		}

		fill_array(arr_dst, 0.0f, N);
	};

	sw.start();

	fmadd_1_wide(arr_a, arr_b, arr_c, arr_dst, N);

	report_and_reset(1);

	sw.start();

	fmadd_4_wide(arr_a, arr_b, arr_c, arr_dst, N);

	report_and_reset(4);

	sw.start();

	fmadd_8_wide(arr_a, arr_b, arr_c, arr_dst, N);

	report_and_reset(8);

	free(arr_a);
	free(arr_b);
	free(arr_c);
	free(arr_dst);
}
```

![alt text](https://github.com/adam-lafontaine/CMS/raw/master/img/%5B007%5D/simd_fma.png)

All versions are slower than then their multiply counterparts.  The fused multiply-add is technically one operation but it is clearly more work for the processor to perform.  With compiler optimizations, the 1 and 4 wide implementations are faster than the 8 wide implementation.  8 wide is still the same speed with our without compiler optimizations.


### Example - SIMD Hypotenuse 3D

The above examples were fairly simple operations.  What happens when operations require more work and there is no SIMD function to perform it all in one step?

Calculating a three dimensional hypotenuse has several steps.  First is to multiply each dimension with itself to square the values.  Then, add the squares together.  Lastly, take the square root.

```cpp
#include <cmath>


void hypot_1_wide(r32* arr_a, r32* arr_b, r32* arr_c, r32* arr_dst, size_t N)
{
	for (size_t i = 0; i < N; ++i)
	{
		auto asq = arr_a[i] * arr_a[i];
		auto bsq = arr_b[i] * arr_b[i];
		auto csq = arr_c[i] * arr_c[i];

		arr_dst[i] = sqrtf(asq + bsq + csq);
	}
}
```

Not so simple in SIMD, but doable.


```cpp
void hypot_4_wide(r32* arr_a, r32* arr_b, r32* arr_c, r32* arr_dst, size_t N)
{
	__m128 wa;
	__m128 wb;
	__m128 wc;
	__m128 wdst;

	for (size_t i = 0; i < N; i += 4)
	{
		auto a = arr_a + i;
		auto b = arr_b + i;
		auto c = arr_c + i;
		auto dst = arr_dst + i;

		wa = _mm_load_ps(a);
		wb = _mm_load_ps(b);
		wc = _mm_load_ps(c);

		wa = _mm_mul_ps(wa, wa);
		wb = _mm_mul_ps(wb, wb);
		wc = _mm_mul_ps(wc, wc);

		wdst = _mm_sqrt_ps(_mm_add_ps(_mm_add_ps(wa, wb), wc));

		_mm_store_ps(dst, wdst);
	}
}


void hypot_8_wide(r32* arr_a, r32* arr_b, r32* arr_c, r32* arr_dst, size_t N)
{
	__m256 wa;
	__m256 wb;
	__m256 wc;
	__m256 wdst;

	for (size_t i = 0; i < N; i += 8)
	{
		auto a = arr_a + i;
		auto b = arr_b + i;
		auto c = arr_c + i;
		auto dst = arr_dst + i;

		wa = _mm256_load_ps(a);
		wb = _mm256_load_ps(b);
		wc = _mm256_load_ps(c);

		wa = _mm256_mul_ps(wa, wa);
		wb = _mm256_mul_ps(wb, wb);
		wc = _mm256_mul_ps(wc, wc);

		wdst = _mm256_sqrt_ps(_mm256_add_ps(_mm256_add_ps(wa, wb), wc));

		_mm256_store_ps(dst, wdst);
	}
}
```

Generate the timing output in the same manner as before.

```cpp
void hypotenuse_3d()
{
	printf("\nHypotenuse 3D\n");

	Stopwatch sw;
	r64 time_ms = 0.0;

	size_t const N = 80'000'000;

	auto arr_a = (r32*)malloc(N * sizeof(r32));
	auto arr_b = (r32*)malloc(N * sizeof(r32));
	auto arr_c = (r32*)malloc(N * sizeof(r32));
	auto arr_dst = (r32*)malloc(N * sizeof(r32));

	fill_array(arr_a, 3.0f, N);
	fill_array(arr_b, 2.0f, N);
	fill_array(arr_c, 1.0f, N);
	fill_array(arr_dst, 0.0f, N);

	auto const report_and_reset = [&](int width)
	{
		time_ms = sw.get_time_milli();
		printf("%d wide time: %f\n", width, time_ms);

		for (size_t i = 0; i < N; ++i)
		{
			if (fabs(arr_dst[i] * arr_dst[i] - 14.0f) > 0.00001f)
			{
				printf("!!! hypotenuse error !!!\n");
				break;
			}
		}

		fill_array(arr_dst, 0.0f, N);
	};

	sw.start();

	hypot_1_wide(arr_a, arr_b, arr_c, arr_dst, N);

	report_and_reset(1);

	sw.start();

	hypot_4_wide(arr_a, arr_b, arr_c, arr_dst, N);

	report_and_reset(4);

	sw.start();

	hypot_8_wide(arr_a, arr_b, arr_c, arr_dst, N);

	report_and_reset(8);

	free(arr_a);
	free(arr_b);
	free(arr_c);
	free(arr_dst);
}
```

![alt text](https://github.com/adam-lafontaine/CMS/raw/master/img/%5B007%5D/simd_hypot3d.png)

The calculation takes considerably more time when compiled without optimizations.  With optimizations, the times are the same as those for the fused multiply-add.  The compiler-optimized 8 wide version is also slower than the 1 and 4 wide versions but faster than the 8 wide fused multiply-add.

### SIMD Recap

From a production standpoint, the above excercise has been a complete waste of time.  In all cases the compiler-optimized, 1 wide implementation was the fastest.  This would have been the default scenario anyway.  After refactoring to process multiple elements at once, we made the code slightly slower as well as more complicated and less portable.  The vectorized functions were faster as expected when compiled as is, but optimizations done by the compiler make these performance gains irrelevant.  This is not to be expected in every case though.  These examples are very simple compared to many real world applications.  


### SOA - Struct of Arrays

SOA refers to structuring your data in such a way as to allow the compiler and CPU to process the code and data more efficiently.  The idea is instead of having an array of structs containing properties to do work on, have one struct containing an array for each property.  This can be less intuitive for the developer but it allows the compiler to better optimize the code and the processor can cache data more effectively, reducing the number of times it needs to fetch data from RAM.  

Suppose we have an array of points that we want to plot and a function that draws to the screen given an x and y position.  We would implement it like so.

```cpp
class Point
{
public:
    int x;
    int y;
};


void plot_graph(Point* pts, size_t n_points)
{
    for(size_t i = 0; i < n_points; ++i)
    {
        draw_to_screen(pts[i].x, pts[i].y);
    }
}
```

This is the array of structs approach.  We iterate over an array of Point structs to process their elements.

Alternatively we can have a struct that contains an array of x values and an array of y values and iterate over their elements.

```cpp
class PointSOA
{
public:
    int* x_values;
    int* y_values;

    size_t n_points;
};


void plot_graph(PointSOA const& pts)
{
    for(size_t i = 0; i < pts.n_points; ++i)
    {
        draw_to_screen(pts.x_values[i], pts.y_values[i]);
    }
}
```

This is the struct of arrays (SOA) approach and should be faster.


### Example - SOA Multiply-Add

Our example is similar to the SIMD fused multiply-add example.  

The struct contains the values for the calculation as well as the result.

```cpp
class MultiplyAdd
{
public:
	r32 a = 2.0f;
	r32 b = 3.0f;
	r32 c = 1.0f;
	r32 dst = 0.0f;
};


void madd_array_of_structs()
{
	printf("\nMultiply-Add Array of Structs\n");

	Stopwatch sw;
	r64 time_ms = 0.0;

	size_t const N = 80'000'000;

	sw.start();
	for (size_t i = 0; i < N; ++i)
	{
		struct_array[i].dst = struct_array[i].a* struct_array[i].b + struct_array[i].c;
	}

	time_ms = sw.get_time_milli();
	printf("array of structs time: %f\n", time_ms);

	free(struct_array);
}
```

The struct of arrays holds pointers for the arrays of values.

```cpp

class MultiplyAddSOA
{
public:
	r32* arr_a;
	r32* arr_b;
	r32* arr_c;
	r32* arr_dst;
};


void madd_struct_of_arrays()
{
	printf("\nMultiply-Add Struct of Arrays\n");

	Stopwatch sw;
	r64 time_ms = 0.0;

	size_t const N = 80'000'000;

	MultiplyAddSOA madd_soa;
	madd_soa.arr_a = (r32*)malloc(N * sizeof(r32));
	madd_soa.arr_b = (r32*)malloc(N * sizeof(r32));
	madd_soa.arr_c = (r32*)malloc(N * sizeof(r32));
	madd_soa.arr_dst = (r32*)malloc(N * sizeof(r32));

	fill_array(madd_soa.arr_a, 3.0f, N);
	fill_array(madd_soa.arr_b, 2.0f, N);
	fill_array(madd_soa.arr_c, 1.0f, N);
	fill_array(madd_soa.arr_dst, 0.0f, N);

	sw.start();
	for (size_t i = 0; i < N; ++i)
	{
		madd_soa.arr_dst[i] = madd_soa.arr_a[i] * madd_soa.arr_b[i] + madd_soa.arr_c[i];
	}

	time_ms = sw.get_time_milli();
	printf("struct of arrays time: %f\n", time_ms);

	free(madd_soa.arr_a);
	free(madd_soa.arr_b);
	free(madd_soa.arr_c);
	free(madd_soa.arr_dst);
}
```

![alt text](https://github.com/adam-lafontaine/CMS/raw/master/img/%5B007%5D/soa_fma.png)

The SOA version is significantly faster with and without compiler optimizations.  The speeds match the 1 wide implementations of the SIMD fused multiply-add example.  This is because the implementations are exactly the same.  Data elements are contiguous, which makes it possible to vectorize and cache efficiently.


### SOA Recap

It seems that the biggest bang for your buck can be achieved by arranging data in memory that is optimal for the CPU to process.  It's easy to forget that computers are physical devices in the real world and data exists in physical memory.  Where and how the memory is arranged does matter.  Software is more than just code and abstractions.  It is instructions for a CPU to follow.  When completing a given task, some instructions can be better than others.


### Should we optimize?

This is the question that we should always be asking ourselves rather than simply optimizing for its own sake or never bothering to in order to save time.  It requires an investement of time and effort and each situation will have different pros and cons to consider.  The SIMD examples in this post did not show any performance gains but their operations are quite simple.  It's likely that the compiler optimizations did them for us anyway.  There are certainly situations where it is worth while, otherwise why would it exist?  

Here is an example doing edge detection on grayscale images.  The numbers are relative times per pixel rather than raw milliseconds.  This allows for comparing different sizes of images and different quantities.

![alt text](https://github.com/adam-lafontaine/CMS/raw/master/img/%5B007%5D/image_processing.png)

* seq = Raw loops or std::for_each(...)
* par = Parallel processing using std::execution::par
* simd = 4 wide intel SIMD instrinsics

Here the SIMD implementations made a difference though not as much as using the parallelized standard algorithms.  I could try other optimizations.  I could use 8 wide intrinsics.  I could parallelize the SIMD operations.  I could apply the SOA principle and use planar color images instead of interleaved RGBA channels.  Eventually I will try these as an excercise because that is the purpose of my image processing library.  They will take more time and cause more headaches.  

Sometimes the end result will be worth the effort and sometimes it won't.  Sometimes we won't find out if the extra effort was worthwhile until after the fact.


### First make it work, and then make it better

The main objective should always be to solve the problem.  There is no point in trying to figure out the optimal implementation at the start because we won't understand the problem until after we have solved it.  Solve the problem first.  Improve it afterwards if necessary.  At least a working solution will be available for people to use while an improved version is being developed.  Remember that nothing is ever perfect so there will always be improvements to be made.  The key is to know when to stop and move on.